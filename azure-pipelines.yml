# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

trigger:
- master

pool:
  name: Default
steps:
- bash: |
    export PYTHONUNBUFFERED=1
    echo $USER
    set +x
    stf --docker-image-dont-pull test --sch-server-url '$(DEV_SCH_URL)' --sch-username '$(DEV_SCH_USER)' --sch-password "$(DEV_SCH_PASSWORD)"  --pipeline_id "$(DEV_PIPELINE_ID)" --elasticsearch-url '$(DEV_ELASTICSEARCH_URL)' --cluster-server '$(DEV_KAFKA_URL)' --kafka-version 2.7.0 --kafka-zookeeper $(DEV_ZK_URL) -vv --upgrade-jobs --junit-xml=/root/tests/output/test-output.xml
    sudo chown -R azureuser:azureuser /home/azureuser/myagent
  displayName: 'Run Tests on Streamsets Pipeline'
- task: PublishTestResults@2
  condition: succeeded()
  inputs:
    testResultsFiles: '**/test-*.xml'
    testRunTitle: 'Publish test results for Kafka To ElasticSearch Streamsets Pipeline'
    failTaskOnFailedTests: true
- task: PythonScript@0
  condition: succeeded()
  inputs:
    scriptSource: filePath
    scriptPath: export_pipeline_from_dev.py
    pythonInterpreter: /usr/bin/python3
    arguments: --pipeline_id "$(DEV_PIPELINE_ID)" --dev_sch_url "$(DEV_SCH_URL)" --dev_sch_user "$(DEV_SCH_USER)" --dev_sch_password "$(DEV_SCH_PASSWORD)" 

- bash: |
    mv *.json "$(QA_PIPELINE_NAME).json"
    for file in *.json; do mv "$file" `echo $file | tr ' ' '_'` ; done
    git clone git@github.com:streamsets/Prasanna_Pipeline_Exports
    mv *.json Prasanna_Pipeline_Exports
    git add -A
    git commit -m "Azure Pipeline commit"
    git push
  displayName: 'Checkin the pipeline json to GIT'
